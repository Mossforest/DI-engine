wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.14.0
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.8.5
    start_time: 1687839278.51045
    t:
      1:
      - 1
      - 5
      - 53
      - 55
      2:
      - 1
      - 5
      - 53
      - 55
      3:
      - 2
      - 16
      - 23
      4: 3.8.5
      5: 0.14.0
      8:
      - 5
env:
  desc: null
  value:
    collector_env_num: 8
    env_id: AsterixNoFrameskip-v4
    evaluator_env_num: 8
    frame_stack: 4
    import_names:
    - dizoo.atari.envs.atari_env
    manager:
      auto_reset: true
      cfg_type: SyncSubprocessEnvManagerDict
      connect_timeout: 60
      context: fork
      copy_on_get: true
      episode_num: .inf
      max_retry: 5
      reset_inplace: false
      reset_timeout: null
      retry_type: reset
      retry_waiting_time: 0.1
      shared_memory: true
      step_timeout: null
      step_wait_timeout: null
      type: subprocess
      wait_num: .inf
    n_evaluator_episode: 8
    stop_value: 10000000000
    type: atari
exp_name:
  desc: null
  value: asterix_dqn_seed0_230627_041431
policy:
  desc: null
  value:
    bp_update_sync: true
    cfg_type: DQNPolicyDict
    collect:
      collector:
        cfg_type: SampleSerialCollectorDict
        collect_print_freq: 100
        deepcopy_obs: false
        transform_obs: false
        type: sample
      n_sample: 100
      unroll_len: 1
    cuda: true
    discount_factor: 0.99
    eval:
      evaluator:
        cfg_type: InteractionSerialEvaluatorDict
        eval_freq: 20000
        n_episode: 8
        render:
          mode: train_iter
          render_freq: -1
        stop_value: 10000000000
    learn:
      batch_size: 32
      ignore_done: false
      learner:
        cfg_type: BaseLearnerDict
        dataloader:
          num_workers: 0
        hook:
          load_ckpt_before_run: ''
          log_show_after_iter: 100
          save_ckpt_after_iter: 10000
          save_ckpt_after_run: true
        log_policy: true
        train_iterations: 1000000000
      learning_rate: 0.00025
      target_theta: 0.005
      target_update_freq: 500
      train_iterations: 12000000
      update_per_collect: 4
    model:
      action_shape: 9
      encoder_hidden_size_list:
      - 128
      - 128
      - 512
      obs_shape:
      - 4
      - 84
      - 84
    multi_gpu: false
    nstep: 1
    on_policy: false
    other:
      commander:
        cfg_type: BaseSerialCommanderDict
      eps:
        decay: 1000000
        end: 0.1
        start: 1.0
        type: linear
      replay_buffer:
        alpha: 0.6
        anneal_step: 100000
        beta: 0.4
        cfg_type: AdvancedReplayBufferDict
        deepcopy: false
        enable_track_used_data: false
        max_staleness: .inf
        max_use: .inf
        monitor:
          periodic_thruput:
            seconds: 60
          sampled_data_attr:
            average_range: 5
            print_freq: 200
        replay_buffer_size: 100000
        thruput_controller:
          push_sample_rate_limit:
            max: .inf
            min: 0
          sample_min_limit_ratio: 1
          window_seconds: 30
        type: advanced
    priority: false
    priority_IS_weight: false
    traj_len_inf: false
    type: dqn
seed:
  desc: null
  value: 0
